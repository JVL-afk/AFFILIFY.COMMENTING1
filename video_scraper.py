import asyncio
import random
import json
import os
from datetime import datetime
from typing import List, Dict, Optional
from fake_useragent import UserAgent
from logger_system import log_start, log_end, log_error, affilify_logger

class MilitaryGradeVideoScraper:
    """
    ROLEX-GRADE STEALTH SCRAPER - NODRIVER EDITION
    Bypasses bot detection using:
    - Nodriver (most undetected automation)
    - SadCaptcha integration (automatic captcha solving)
    - Account cookie injection (Logged-in state)
    - Randomized human behavior
    - Trending feed fallback
    """
    
    def __init__(self):
        self.ua = UserAgent()
        self.logger = affilify_logger.main_logger
        self.cookie_dir = "/home/ubuntu/AFFILIFY.COMMENTING1/affilify_data/cookies"
        self.targets = []
        self.driver = None
        self.tab = None

    async def _get_random_cookie_file(self):
        """Get a random cookie file from the available accounts."""
        if not os.path.exists(self.cookie_dir):
            return None
        files = [f for f in os.listdir(self.cookie_dir) if f.endswith('.json')]
        return os.path.join(self.cookie_dir, random.choice(files)) if files else None
    
    async def _load_cookies(self, cookie_file):
        """Load and fix cookies from a file."""
        try:
            with open(cookie_file, 'r') as f:
                data = json.load(f)
                # Handle the specific structure: {"username": "...", "cookies": [...]}
                if isinstance(data, dict) and "cookies" in data:
                    cookies = data["cookies"]
                else:
                    cookies = data
                            
            self.logger.info(f"ðŸ‘¤ Loaded {len(cookies)} cookies from: {os.path.basename(cookie_file)}")
            return cookies
        except Exception as e:
            self.logger.error(f"âŒ Failed to load cookies: {e}")
            return []

    async def _init_nodriver_context(self):
        """Initialize Nodriver browser with SadCaptcha integration."""
        api_key = os.getenv("SADCAPTCHA_API_KEY")
        
        if not api_key:
            self.logger.error("âŒ SADCAPTCHA_API_KEY not found in environment!")
            raise ValueError("SADCAPTCHA_API_KEY is required")
        
        try:
            from tiktok_captcha_solver.launcher import make_nodriver_solver
            
            self.logger.info("ðŸ”‘ Initializing Nodriver with SadCaptcha (this may take 10-15 seconds)...")
            
            # Nodriver with SadCaptcha - NO headless flag for better detection bypass
            # Empty browser_args = non-headless mode (best for TikTok)
            self.driver = await asyncio.wait_for(
                make_nodriver_solver(api_key, browser_args=[]),
                timeout=30.0
            )
            
            self.logger.info("âœ… Nodriver with SadCaptcha initialized successfully!")
            
            # Get the main tab
            self.tab = self.driver.main_tab
            
            # Load cookies
            cookie_file = await self._get_random_cookie_file()
            if cookie_file:
                cookies = await self._load_cookies(cookie_file)
                if cookies:
                    await self._inject_cookies(cookies)
            
            self.logger.info("ðŸŽ¯ SadCaptcha is active and will automatically solve captchas!")
            
            return self.driver, self.tab
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize Nodriver: {e}")
            import traceback
            traceback.print_exc()
            raise

    async def _inject_cookies(self, cookies):
        """Inject cookies into Nodriver using CDP."""
        try:
            for cookie in cookies:
                try:
                    await self.tab.send(self.tab.cdp.network.set_cookie(
                        name=cookie['name'],
                        value=cookie['value'],
                        domain=cookie.get('domain', '.tiktok.com'),
                        path=cookie.get('path', '/'),
                        secure=cookie.get('secure', True),
                        http_only=cookie.get('httpOnly', False),
                        same_site=cookie.get('sameSite', 'Lax')
                    ))
                except:
                    pass  # Skip invalid cookies
            self.logger.info(f"âœ… Injected {len(cookies)} cookies")
        except Exception as e:
            self.logger.error(f"âŒ Cookie injection failed: {e}")

    async def discover_targets_comprehensive(self, max_videos: int = 150):
        """Main entry point for target discovery with Nodriver and SadCaptcha."""
        start_time = log_start("DiscoverTargets_Nodriver")
        niches = ["affiliate marketing", "passive income", "side hustle", "e-commerce tips", 
                  "dropshipping", "amazon fba", "digital marketing", "money making", "business ideas"]
        random.shuffle(niches)
        all_videos = []
        
        try:
            # Initialize Nodriver with SadCaptcha
            driver, tab = await self._init_nodriver_context()
            
            try:
                # Attempt 1: Hashtag-based discovery (SCALE UP for ROLEX GRADE filtering)
                hashtags = [
                    "affiliatemarketing", "passiveincome", "sidehustle", "dropshipping", 
                    "money", "makemoneyonline", "digitalmarketing", "ecommerce",
                    "entrepreneur", "businesstips", "onlinebusiness", "workfromhome"
                ]
                random.shuffle(hashtags)
                
                # Search MORE hashtags to get 100+ videos for filtering
                for tag in hashtags[:12]:  # Search up to 12 hashtags for ROLEX GRADE
                    self.logger.info(f"ðŸ·ï¸ Searching hashtag: #{tag}")
                    tag_videos = await self._search_hashtag_nodriver(tab, tag)
                    all_videos.extend(tag_videos)
                    self.logger.info(f"ðŸ“Š Total videos collected: {len(all_videos)}")
                    
                    # Continue until we have enough for filtering
                    if len(all_videos) >= max_videos: 
                        self.logger.info(f"âœ… Reached target of {max_videos} videos")
                        break
                    
                    await asyncio.sleep(random.uniform(3, 5))  # Faster between searches

                # Attempt 2: Trending/Home Feed discovery (Fallback)
                if len(all_videos) < 5:
                    self.logger.info("ðŸ“¡ Activating Feed-Based Discovery...")
                    feed_videos = await self._get_trending_fallback_nodriver(tab)
                    all_videos.extend(feed_videos)
                
                # Attempt 3: Search-based discovery (Last resort)
                if len(all_videos) < 5:
                    for niche in niches[:2]:
                        self.logger.info(f"ðŸ” Searching niche: {niche}")
                        videos = await self._search_niche_nodriver(tab, niche)
                        all_videos.extend(videos)
                        await asyncio.sleep(random.uniform(3, 5))
                
                # Deduplicate and filter
                unique_videos = {v['video_url']: v for v in all_videos}.values()
                self.targets = list(unique_videos)[:max_videos]
                
                # ROLEX FALLBACK: If everything is blocked, provide simulated targets
                if not self.targets:
                    self.logger.warning("âš ï¸ All discovery methods blocked. Activating ROLEX fallback...")
                    self.targets = self._generate_simulated_targets(max_videos)
                
                self.logger.info(f"âœ… Discovery complete: {len(self.targets)} targets acquired")
                log_end("DiscoverTargets_Nodriver", start_time, True)
                return self.targets
                
            finally:
                # Clean up
                if driver:
                    driver.stop()
                    
        except Exception as e:
            log_error("DiscoverTargets_Nodriver", e)
            self.logger.error(f"âŒ Discovery failed: {e}")
            # Return simulated targets as fallback
            self.targets = self._generate_simulated_targets(max_videos)
            return self.targets

    async def _search_hashtag_nodriver(self, tab, hashtag: str) -> List[Dict]:
        """Search TikTok by hashtag using Nodriver with scrolling for more videos."""
        try:
            url = f"https://www.tiktok.com/tag/{hashtag}"
            await tab.get(url)
            
            # Wait for page load
            await asyncio.sleep(3)
            
            # Check for captcha
            page_source = await tab.get_content()
            if "captcha" in page_source.lower() or "verify" in page_source.lower():
                self.logger.info(f"ðŸ›¡ï¸ Captcha detected for #{hashtag}. SadCaptcha extension will handle it...")
                # Wait longer for SadCaptcha to solve (increased from 10s to 60s)
                await asyncio.sleep(60)
                # Get updated page source
                page_source = await tab.get_content()
            
            # Extract videos from initial page
            all_videos = await self._extract_video_elements_nodriver(tab, page_source)
            
            # Scroll down 2-3 times to load more videos
            for scroll_num in range(3):
                try:
                    # Scroll to bottom
                    await tab.scroll_down(1000)
                    await asyncio.sleep(2)  # Wait for content to load
                    
                    # Get new page source
                    page_source = await tab.get_content()
                    new_videos = await self._extract_video_elements_nodriver(tab, page_source)
                    
                    # Add only new unique videos
                    existing_ids = {v['video_id'] for v in all_videos}
                    for video in new_videos:
                        if video['video_id'] not in existing_ids:
                            all_videos.append(video)
                            existing_ids.add(video['video_id'])
                    
                    self.logger.info(f"ðŸ“œ Scroll {scroll_num+1}: Total {len(all_videos)} unique videos")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Scroll {scroll_num+1} failed: {e}")
                    break
            
            self.logger.info(f"âœ… Found {len(all_videos)} videos for #{hashtag}")
            return all_videos
            
        except Exception as e:
            self.logger.error(f"âŒ Hashtag search failed for #{hashtag}: {e}")
            return []

    async def _search_niche_nodriver(self, tab, niche: str) -> List[Dict]:
        """Search TikTok by niche keyword using Nodriver."""
        try:
            # Use search page
            search_query = niche.replace(" ", "%20")
            url = f"https://www.tiktok.com/search?q={search_query}"
            await tab.get(url)
            
            await asyncio.sleep(3)
            
            # Check for captcha
            page_source = await tab.get_content()
            if "captcha" in page_source.lower() or "verify" in page_source.lower():
                self.logger.info(f"ðŸ›¡ï¸ Captcha detected for search '{niche}'. SadCaptcha extension will handle it...")
                await asyncio.sleep(60)
                page_source = await tab.get_content()
            
            videos = await self._extract_video_elements_nodriver(tab, page_source)
            self.logger.info(f"âœ… Found {len(videos)} videos for niche '{niche}'")
            return videos
            
        except Exception as e:
            self.logger.error(f"âŒ Niche search failed for '{niche}': {e}")
            return []

    async def _get_trending_fallback_nodriver(self, tab) -> List[Dict]:
        """Get videos from trending/home feed using Nodriver."""
        try:
            # Try home feed first
            await tab.get("https://www.tiktok.com/foryou")
            await asyncio.sleep(5)
            
            page_source = await tab.get_content()
            videos = await self._extract_video_elements_nodriver(tab, page_source)
            
            if not videos:
                self.logger.info("ðŸ“¡ Home feed empty, trying Explore...")
                await tab.get("https://www.tiktok.com/explore")
                await asyncio.sleep(5)
                page_source = await tab.get_content()
                videos = await self._extract_video_elements_nodriver(tab, page_source)
            
            return videos
            
        except Exception as e:
            self.logger.error(f"âŒ Trending fallback failed: {e}")
            return []

    async def _extract_video_elements_nodriver(self, tab, page_source: str) -> List[Dict]:
        """Extract FULL video metadata from page source (ROLEX GRADE)."""
        videos = []
        
        try:
            import re
            import json
            
            # Try to extract from __UNIVERSAL_DATA_FOR_REHYDRATION__ (TikTok's main data object)
            universal_data_pattern = r'<script id="__UNIVERSAL_DATA_FOR_REHYDRATION__" type="application/json">(.*?)</script>'
            universal_match = re.search(universal_data_pattern, page_source, re.DOTALL)
            
            if universal_match:
                try:
                    data = json.loads(universal_match.group(1))
                    self.logger.info("ðŸ” Found TikTok universal data object")
                    
                    # Extract videos from the data structure
                    videos_extracted = self._parse_tiktok_universal_data(data)
                    if videos_extracted:
                        self.logger.info(f"âœ… Extracted {len(videos_extracted)} videos with FULL metadata")
                        return videos_extracted
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Failed to parse universal data: {e}")
            
            # Fallback: Extract video IDs only and mark for metadata enrichment
            self.logger.info("ðŸ“‹ Falling back to ID extraction (will need metadata enrichment)")
            
            # Pattern 1: Direct video URLs
            video_pattern = r'https://www\.tiktok\.com/@([\w.-]+)/video/(\d+)'
            matches = re.findall(video_pattern, page_source)
            
            # Pattern 2: Video IDs in data attributes
            if not matches:
                id_pattern = r'"id":"(\d{19})"'
                id_matches = re.findall(id_pattern, page_source)
                matches = [('unknown', vid) for vid in id_matches]
            
            # Filter to ensure they're real TikTok video IDs (19 digits)
            valid_videos = [(author, vid) for author, vid in matches if len(str(vid)) == 19]
            
            self.logger.info(f"âœ… Found {len(valid_videos)} valid video IDs (metadata needed)")
            
            for author, video_id in valid_videos[:50]:  # Increased to 50 per page
                videos.append({
                    'video_url': f'https://www.tiktok.com/@{author}/video/{video_id}',
                    'video_id': video_id,
                    'author': author,
                    'needs_metadata': True,  # Flag for enrichment
                    'views': 0,
                    'likes': 0,
                    'comments': 0,
                    'shares': 0,
                    'timestamp': datetime.now().isoformat()
                })
            
        except Exception as e:
            self.logger.error(f"âŒ Video extraction failed: {e}")
            import traceback
            traceback.print_exc()
        
        return videos
    
    def _parse_tiktok_universal_data(self, data: dict) -> List[Dict]:
        """Parse TikTok's universal data object to extract video metadata."""
        videos = []
        
        try:
            # Navigate the complex nested structure
            # TikTok stores video data in: __DEFAULT_SCOPE__['webapp.video-detail'] or similar
            
            if '__DEFAULT_SCOPE__' in data:
                scope = data['__DEFAULT_SCOPE__']
                
                # Try different possible paths
                video_paths = [
                    'webapp.video-detail',
                    'webapp.search-item',
                    'webapp.user-detail',
                    'ItemModule',
                    'ItemList'
                ]
                
                for path in video_paths:
                    if path in scope:
                        items = scope[path]
                        
                        if isinstance(items, dict):
                            # ItemModule structure
                            for video_id, video_data in items.items():
                                if isinstance(video_data, dict) and 'id' in video_data:
                                    video = self._extract_video_from_item(video_data)
                                    if video:
                                        videos.append(video)
                        elif isinstance(items, list):
                            # List structure
                            for item in items:
                                if isinstance(item, dict):
                                    video = self._extract_video_from_item(item)
                                    if video:
                                        videos.append(video)
            
            self.logger.info(f"ðŸ“Š Parsed {len(videos)} videos from universal data")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to parse universal data: {e}")
        
        return videos
    
    def _extract_video_from_item(self, item: dict) -> Optional[Dict]:
        """Extract video metadata from a single item."""
        try:
            video_id = item.get('id', '')
            if not video_id or len(str(video_id)) != 19:
                return None
            
            # Extract author info
            author_data = item.get('author', {}) or item.get('authorInfo', {})
            author = author_data.get('uniqueId', 'unknown')
            author_followers = author_data.get('followerCount', 0) or author_data.get('fans', 0)
            author_verified = author_data.get('verified', False)
            
            # Extract stats
            stats = item.get('stats', {}) or item.get('statsV2', {})
            views = stats.get('playCount', 0) or stats.get('viewCount', 0)
            likes = stats.get('diggCount', 0) or stats.get('likeCount', 0)
            comments = stats.get('commentCount', 0)
            shares = stats.get('shareCount', 0)
            
            # Extract description and hashtags
            desc = item.get('desc', '') or item.get('description', '')
            hashtags = [tag.get('name', '') for tag in item.get('textExtra', []) if tag.get('hashtagName')]
            
            # Calculate engagement rate
            engagement = likes + comments + shares
            engagement_rate = (engagement / views * 100) if views > 0 else 0
            
            # Calculate video age
            create_time = item.get('createTime', 0)
            if create_time:
                hours_old = (datetime.now().timestamp() - create_time) / 3600
            else:
                hours_old = 999  # Unknown age
            
            return {
                'video_url': f'https://www.tiktok.com/@{author}/video/{video_id}',
                'video_id': video_id,
                'author': author,
                'creator_username': author,
                'creator_followers': author_followers,
                'creator_verified': author_verified,
                'views': views,
                'likes': likes,
                'comments': comments,
                'shares': shares,
                'description': desc,
                'hashtags': hashtags,
                'engagement_rate': engagement_rate,
                'hours_old': hours_old,
                'timestamp': datetime.now().isoformat(),
                'needs_metadata': False  # Full metadata extracted!
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to extract video from item: {e}")
            return None

    def _generate_simulated_targets(self, count: int = 10) -> List[Dict]:
        """Generate high-quality simulated targets for testing/demo."""
        self.logger.info(f"ðŸŽ­ Generating {count} ROLEX-GRADE simulated targets...")
        
        simulated = []
        for i in range(count):
            video_id = f"7{random.randint(100000000000000000, 999999999999999999)}"
            simulated.append({
                'video_url': f'https://www.tiktok.com/@affilify_demo/video/{video_id}',
                'video_id': video_id,
                'author': f'demo_user_{i+1}',
                'views': random.randint(10000, 500000),
                'likes': random.randint(1000, 50000),
                'comments': random.randint(100, 5000),
                'shares': random.randint(50, 2000),
                'timestamp': datetime.now().isoformat(),
                'simulated': True
            })
        
        return simulated
